{
  "hash": "4c1ba245ac162050cb84b5a372baa90e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Efficient Gaussian Copula Density Computation for Large-Scale Spatial Data: A Matérn-like GMRF Approach with Circulant and Folded Circulant Approximations\"\nauthor: \n  name: Brynjólfur Gauti Guðrúnar Jónsson\n  affiliation: University of Iceland\n  email: brynjolfur@hi.is\n  url: bggj.is\ndate: 2024/08/15\nnumber-sections: false\ncode-block-border-left: true\n---\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.1\nv ggplot2   3.5.1     v tibble    3.2.1\nv lubridate 1.9.3     v tidyr     1.3.1\nv purrr     1.0.2     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(glue)\nlibrary(gt)\n```\n:::\n\n\n\n\n\n\n# Introduction\n\n## Problem Formulation\n\nConsider a spatial field on a regular $n_1 \\times n_2$ grid. Our objective is to compute the Gaussian copula density efficiently for this field. This computation involves:\n\n1. Specifying an $n_1 n_2 \\times n_1 n_2$ precision matrix $\\mathbf{Q}$ that represents the spatial dependence structure.\n2. Ensuring the implied covariance matrix $\\mathbf{\\Sigma} = \\mathbf{Q}^{-1}$ has unit diagonal elements.\n3. Computing the log determinant, $\\log |\\mathbf Q|$, and the quadratic form $z^T \\mathbf Q z$ where $z_i = \\Phi^{-1}(f_i(y_i))$\n\n\n\n## Review\n\nGaussian Markov Random Fields (GMRFs) and copulas are two powerful statistical tools, each offering unique strengths in modeling complex data structures. GMRFs excel in capturing spatial and temporal dependencies, particularly in fields such as environmental science, epidemiology, and image analysis. Their ability to represent local dependencies through sparse precision matrices makes them computationally attractive for high-dimensional problems. Copulas, on the other hand, provide a flexible framework for modeling multivariate dependencies, allowing separate specification of marginal distributions and their joint behavior.\n\nThe Gaussian copula, in particular, has gained popularity due to its interpretability and connection to the multivariate normal distribution. However, combining GMRFs with copulas has historically been computationally challenging, limiting their joint application to smaller datasets or simpler models.\n\nLet $\\mathbf{X} = (X_1, X_2, \\ldots, X_n)$ be a multivariate random vector with marginal distribution functions $F_i$ for $i = 1, 2, \\ldots, n$. The joint distribution function of $\\mathbf{X}$ can be written as:\n\n$$\nF_{\\mathbf{X}}(\\mathbf{x}) = C(F_1(x_1), F_2(x_2), \\ldots, F_n(x_n)),\n$$\n\nwhere $C$ is the Gaussian copula defined by the GMRF precision matrix $\\mathbf{Q}$. The Gaussian copula $C$ is given by:\n\n$$\nC(u_1, u_2, \\ldots, u_n) = \\Phi_\\mathbf{Q}(\\Phi^{-1}(u_1), \\Phi^{-1}(u_2), \\ldots, \\Phi^{-1}(u_n)),\n$$\n\nwhere $\\Phi_\\mathbf{Q}$ is the joint cumulative distribution function of a multivariate normal distribution with mean vector $\\mathbf{0}$ and precision matrix $\\mathbf{Q}$, and $\\Phi^{-1}$ is the inverse of the standard normal cumulative distribution function.\n\nA critical requirement for the precision matrix $\\mathbf{Q}$ governing the GMRF copula $C$ is that $\\mathbf{\\Sigma} = \\mathbf{Q}^{-1}$ should have a unit diagonal, i.e. the marginal variance is equal to one everywhere.. This ensures it operates on the same scale as the transformed data, $\\Phi^{-1}(u_i)$. However, this can be challenging as GMRFs are typically defined in terms of precision matrices that often imply non-unit marginal variances.\n\nThis paper presents a novel algorithm that bridges the gap between GMRFs and copulas, allowing for fast and efficient computation of Gaussian copula densities using GMRF precision structures. Our method focuses on creating a Matérn-like precision matrix $\\mathbf{Q}$ with unit marginal variance and efficiently computing the multivariate Gaussian copula density of $\\mathbf{Z} = \\Phi^{-1}(\\mathbf{u})$, where $u_i \\sim \\text{Uniform}(0, 1)$, $i = 1, \\dots, n$.\n\nThe key innovation lies in leveraging the special structure of the precision matrix:\n\n$$\n\\mathbf{Q} = \\mathbf{Q}_{\\rho_1} \\otimes \\mathbf{I_{n_2}} + \\mathbf{I_{n_1}} \\otimes \\mathbf{Q}_{\\rho_2},\n$$\n\nwhere $\\mathbf{Q}_\\rho$ is the precision matrix of a standardized one-dimensional AR(1) process with correlation $\\rho$ and $\\otimes$ denotes the Kronecker product. By employing efficient eigendecomposition techniques, our method avoids explicit formation and inversion of the large precision matrix $\\mathbf{Q}$, making it particularly suitable for high-dimensional spatial data. In addition to the exact method, we show how the precision matrix can be approximated by a folded circulant matrix wich gives a large speed-up while preserving suitable boundary conditions.\n\n# Methods\n\n## Gaussian Copula Density Computation\n\nThe Gaussian copula density for a random vector $\\mathbf{U} = (U_1, ..., U_n)$ with $U_i \\sim \\text{Uniform}(0,1)$ is given by:\n\n$$\nc(\\mathbf{u}) = |\\mathbf{Q}|^{1/2} \\exp\\left(-\\frac{1}{2}\\mathbf{z}^T(\\mathbf{Q} - \\mathbf{I})\\mathbf{z}\\right)\n$$\n\nwhere $\\mathbf{z} = (z_1, ..., z_n)$ with $z_i = \\Phi^{-1}(u_i)$, $\\mathbf{Q}$ is the precision matrix, and $\\mathbf{I}$ is the identity matrix.\n\nThe log-density can be expressed as:\n\n$$\n\\log c(\\mathbf{u}) = \\frac{1}{2}\\log|\\mathbf{Q}| - \\frac{1}{2}\\mathbf{z}^T\\mathbf{Q}\\mathbf{z} + \\frac{1}{2}\\mathbf{z}^T\\mathbf{z}\n$$\n\nOur goal is to efficiently compute this log-density for large spatial fields.\n\n## Precision Matrix Structure\n\nWe define the precision matrix $\\mathbf{Q}$ as:\n\n$$\n\\mathbf{Q} = (\\mathbf{Q}_{\\rho_1} \\otimes \\mathbf{I_{n_2}} + \\mathbf{I_{n_1}} \\otimes \\mathbf{Q}_{\\rho_2})^{(\\nu + 1)}, \\quad \\nu \\in \\{0, 1, 2\\}\n$$\n\nwhere $\\mathbf{Q}_\\rho$ is the precision matrix of a one-dimensional AR(1) process with correlation $\\rho$:\n\n$$\n\\mathbf{Q}_\\rho = \\frac{1}{1-\\rho^2}\n\\begin{bmatrix}\n1 & -\\rho & 0 & \\cdots & 0 \\\\\n-\\rho & 1+\\rho^2 & -\\rho & \\cdots & 0 \\\\\n0 & -\\rho & 1+\\rho^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1\n\\end{bmatrix}\n$$\n\nThe matrix, $\\mathbf Q$, is then scaled so that its inverse, $\\mathbf \\Sigma = \\mathbf Q^{-1}$ is a correlation matrix, i.e. $\\mathbf \\Sigma_{ii} = 1$.\n\n## Computation Process\n\n### Step 1: Eigendecomposition of $\\mathbf{Q}_{\\rho}$\n\nWe first compute the eigendecomposition of both $\\mathbf{Q}_{\\rho}$$:\n\n$$\n\\mathbf{Q}_{\\rho} = \\mathbf{V_{\\rho}}\\mathbf{\\Lambda_\\rho}\\mathbf{V_\\rho}^T\n$$\n\nwhere $\\mathbf{V_\\rho}$ is the matrix of eigenvectors and $\\mathbf{\\Lambda_\\rho}$ is the diagonal matrix of eigenvalues. Then, because of how $Q$ is defined, its eigendecomposition is:\n\n$$\n\\mathbf{Q} = (\\mathbf{V_{\\rho_1}} \\otimes \\mathbf{V_{\\rho_2}})(\\mathbf{\\Lambda_{\\rho_1}} \\otimes \\mathbf{I} + \\mathbf{I} \\otimes \\mathbf{\\Lambda_{\\rho_2}})^{(\\nu + 1)}(\\mathbf{V_{\\rho_1}} \\otimes \\mathbf{V_{\\rho_2}})^T.\n$$\n\nWe don't work with the full eigendecomposition, but rather utilize the fact that the eigenvalues of $\\mathbf Q$ are $\\left\\{\\lambda_{\\rho_1}\\right\\}_i + \\left\\{\\lambda_{\\rho_2}\\right\\}_j$ and their corresponding eigenvectors are $\\left\\{\\mathbf{v}_{\\rho_1}\\right\\}_i \\otimes \\left\\{\\mathbf{v}_{\\rho_2}\\right\\}_j$ to iterate over each value and vector pair to compute the density without forming the larger matrix.\n\n### Step 2: Computation of Marginal Standard Deviations\n\nIn order to scale $\\mathbf Q$ so that its inverse is a correlation matrix, we first calculate $\\sigma_i = \\sqrt\\Sigma_{ii}$, $i = 1, \\dots, n_1n_2$. We then use these marginal standard deviations to scale the eigenvectors and values. The inverse of $Q$ is given by:\n\n$$\n\\boldsymbol \\Sigma = \\mathbf Q^{-1} = (VAV^T)^{-1} = VA^{-1}V\n$$\n\nThe diagonal elements, $\\boldsymbol \\Sigma_{ii}$, are given by:\n\n$$\n\\Sigma_{ii} = \\sum_{k=1}^{n} v_{ik} \\frac{1}{\\lambda_k} (v^T)_{ki} = \\sum_{k=1}^{n} v_{ik} \\frac{1}{\\lambda_k} v_{ik} = \\sum_{k=1}^{n} v_{ik}^2 \\frac{1}{\\lambda_k}\n$$\n\nThis means that the $i$'th marginal variance, $\\sigma_i^2$, is a weighted sum of the reciprocals of the eigenvalues of $\\mathbf Q$ where the weights are the squares of the $i$'th value in each eigenvector. This means that we can calculate the marginal standard deviations by iterating over the eigenvalues and -vectors of $Q_{\\rho_1}$ and $Q_{\\rho_2}$ and cumulating their values according to the formula above, then taking the element-wise square roots.\n\n\n### Step 3: Scaling the Eigendecomposition\n\nTo scale the eigendecomposition of $\\mathbf{Q}$ using the marginal standard deviations, we define a diagonal matrix $\\mathbf{D}$, where $D_{ii} = \\sigma_i$ and scale the precision matrix as:\n\n$$\n\\begin{aligned}\n\\mathbf{\\widetilde  Q} &= \\mathbf{D}\\mathbf{Q}^{\\nu+1}\\mathbf{D} \\\\\n&= \\mathbf{D}\\mathbf{V}\\mathbf{\\Lambda}^{\\nu+1}\\mathbf{V}^T\\mathbf{D} \\\\\n&= \\mathbf{\\widetilde V}\\mathbf{\\widetilde\\Lambda}\\mathbf{\\widetilde V}^T.\n\\end{aligned}\n$$\n\nIn practice, we don't scale the whole eigendecomposition. Instead, we rescale each value/vector pair individually as we iterate over the eigenvectors and values of $Q_{\\rho_1}$ and $Q_{\\rho_2}$ to create the corresponding values and vectors for the larger matrix.\n\n### Step 4: Efficient Computation of Log-Density\n\nUsing this scaled eigendecomposition, we efficiently compute:\n\n1. Log-determinant: $\\log|\\mathbf{\\widetilde Q}| = \\sum_{i,j} \\log(\\widetilde\\lambda_{ij})$, where $\\widetilde\\lambda_{ij}$ is $\\left( \\left\\{\\lambda_{\\rho_1}\\right\\}_i + \\left\\{\\lambda_{\\rho_2}\\right\\}_j \\right)^{\\nu+1}$ after rescaling with marginal standard deviations.\n\n2. Quadratic form: $\\mathbf{z}^T\\mathbf{\\widetilde Q}\\mathbf{z} = \\sum_{i,j} (\\widetilde\\lambda_{ij}) y_{ij}^2$, where $y_{ij} = \\left(\\left\\{\\mathbf{v}_{\\rho_1}\\right\\}_i \\otimes \\left\\{\\mathbf{v}_{\\rho_2}\\right\\}_j\\right)^T\\mathbf{z}$.\n\nThis approach allows us to calculate the density of the spatial copula by calculating and iterating over the spectral decomposition of the smaller matrices, avoiding the formation of $\\mathbf Q$ alltogether.\n\n## Circulant and Folded Circulant Approximations\n\nWhile the eigendecomposition method provides an exact solution, it can be computationally expensive for very large spatial fields. To address this, we introduce circulant and folded circulant approximations that offer potential computational advantages.\n\n### Circulant Matrices\n\nA circulant matrix $C$ is a special kind of matrix where each row is a cyclic shift of the row above it. It can be fully specified by its first row or column, called the base $c$:\n\n$$\nC = \\begin{pmatrix}\nc_0 & c_1 & c_2 & \\cdots & c_{n-1} \\\\\nc_{n-1} & c_0 & c_1 & \\cdots & c_{n-2} \\\\\nc_{n-2} & c_{n-1} & c_0 & \\cdots & c_{n-3} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nc_1 & c_2 & c_3 & \\cdots & c_0\n\\end{pmatrix} = (c_{j-i \\mod n})\n$$\n\nThe base vector $c$ completely determines the circulant matrix and plays a crucial role in efficient computations. In particular:\n\n1. The eigenvalues of $C$ are given by the Discrete Fourier Transform (DFT) of $c$:\n$$\n\\lambda = \\text{DFT}(c)\n$$\n\n2. Matrix-vector multiplication can be performed using the FFT:\n$$\nCv = \\text{DFT}(\\text{DFT}(c) \\odot \\text{IDFT}(v))\n$$\n\n3. When $C$ is non singular, then the inverse is circulant and thus determined by its base:\n\n$$\n\\frac1n \\text{IDFT}(\\text{DFT}(c)^{-1}).\n$$\n\nThese properties allow for much faster computations than for general matrices.\n\n\n### Block Circulant Matrices\n\nFor two-dimensional spatial fields, we use block circulant matrices with circulant blocks (BCCB). An $Nn \\times Nn$ matrix C is block circulant if it has the form:\n\n$$\nC = \\begin{pmatrix}\nC_0 & C_1 & C_2 & \\cdots & C_{N-1} \\\\\nC_{N-1} & C_0 & C_1 & \\cdots & C_{N-2} \\\\\nC_{N-2} & C_{N-1} & C_0 & \\cdots & C_{N-3} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nC_1 & C_2 & C_3 & \\cdots & C_0\n\\end{pmatrix} = (C_{j-i \\mod N})\n$$\n\nwhere each $C_i$ is itself a circulant $n \\times n$ matrix.\n\nFor a BCCB matrix, we define a base matrix $\\mathbf c$, which is an $n \\times N$ matrix where each column is the base vector of the corresponding circulant block. This base matrix $\\mathbf c$ completely determines the BCCB matrix and is central to efficient computations:\n\n1. The eigenvalues of $C$ are given by the 2D DFT of $\\mathbf c$.\n\n2. Matrix-vector multiplication can be performed using the 2D FFT.\n\n3. When $C$ is non singular, then the inverse is also a BCCB matrix and thus determined by its base matrix.\n\n### Approximations for $Q_{\\rho}$\n\nLet $Q_{\\rho}$ be the precision matrix of a one-dimensional AR(1) process with correlation $\\rho$. The exact form of $Q_{\\rho}$ is:\n\n$$\n\\mathbf{Q}_\\rho = \\frac{1}{1-\\rho^2}\n\\begin{bmatrix}\n1 & -\\rho & 0 & \\cdots & 0 \\\\\n-\\rho & 1+\\rho^2 & -\\rho & \\cdots & 0 \\\\\n0 & -\\rho & 1+\\rho^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1\n\\end{bmatrix}\n$$\n\n#### Circulant Approximation\n\nThe circulant approximation to $Q_\\rho$, denoted as $\\mathbf{Q}_\\rho^{(circ)}$, is:\n\n$$\n\\mathbf{Q}_\\rho^{(circ)} = \\frac{1}{1-\\rho^2}\n\\begin{bmatrix}\n1+\\rho^2 & -\\rho & 0 & \\cdots & 0 & -\\rho \\\\\n-\\rho & 1+\\rho^2 & -\\rho & \\cdots & 0 & 0 \\\\\n0 & -\\rho & 1+\\rho^2 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n-\\rho & 0 & 0 & \\cdots & -\\rho & 1+\\rho^2\n\\end{bmatrix}\n$$\n\nThis approximation treats the first and last observations as neighbors, effectively wrapping the data around a circle.\n\n#### Folded Circulant Approximation\n\nThe folded circulant approximation, $\\mathbf{Q}_\\rho^{(fold)}$, is based on a reflected version of the data. We double the data by reflecting it, giving us the data $x_1,  \\dots, x_n, x_n, \\dots, x_1$. We then model this doubled data with a $2n \\times 2n$ circulant matrix. If written out as an $n \\times n$ matrix, it takes the form:\n\n$$\n\\mathbf{Q}_\\rho^{(fold)} = \\frac{1}{1-\\rho^2}\n\\begin{bmatrix}\n1-\\rho+\\rho^2 & -\\rho & 0 & \\cdots & 0 & 0 \\\\\n-\\rho & 1+\\rho^2 & -\\rho & \\cdots & 0 & 0 \\\\\n0 & -\\rho & 1+\\rho^2 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & -\\rho & 1-\\rho+\\rho^2\n\\end{bmatrix}\n$$\n\nThis approximation modifies the first and last diagonal elements to account for the reflection of the data. As $x_1$ now is the first and last data point, then we avoid the circular dependence from the regular circulant approximation.\n\n### Extension to the Full Q Matrix\n\nFor a two-dimensional spatial field on an $n_1 \\times n_2$ grid, we construct the full precision matrix Q using a Kronecker sum:\n\n$$\n\\mathbf{Q} = \\left( \\mathbf{Q}_{\\rho_1} \\otimes \\mathbf{I_{n_2}} + \\mathbf{I_{n_1}} \\otimes \\mathbf{Q}_{\\rho_2} \\right)^{(\\nu + 1)}, \\quad \\nu \\in \\{0, 1, 2\\}\n$$\n\nwhere $\\otimes$ denotes the Kronecker product, $I_n$ is the $n \\times n$ identity matrix, and $\\nu$ is a smoothness parameter.\n\nWhen we approximate $Q_\\rho$ with a circulant matrix, this Kronecker sum results in a block-circulant matrix with circulant blocks (BCCB). To see this, let's consider the case where $\\nu = 0$ for simplicity:\n\n$$\n\\mathbf{Q} = \\mathbf{Q}_{\\rho_1} \\otimes \\mathbf{I_{n_2}} + \\mathbf{I_{n_1}} \\otimes \\mathbf{Q}_{\\rho_2}\n$$\n\nNow, let the two AR(1) matrices be approximated by a circulant matrix $C$ with base vector $c = [c_, c_1, ..., c_{n-1}]$. Then:\n\n$$\n\\mathbf{Q}_{\\rho_1} \\approx \\mathbf{C_{\\rho_1}} = \\frac{1}{1-\\rho_1^2}\n\\begin{bmatrix}\n1+\\rho_1^2 & -\\rho_1 & 0 & \\cdots & 0 & -\\rho_1 \\\\\n-\\rho_1 & 1+\\rho_1^2 & -\\rho_1 & \\cdots & 0 & 0 \\\\\n0 & -\\rho_1 & 1+\\rho_1^2 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n-\\rho_1 & 0 & 0 & \\cdots & -\\rho_1 & 1+\\rho_1^2\n\\end{bmatrix},\n$$\n\nand $C_{\\rho_2}$ is defined similarly. The Kronecker product $\\mathbf C_{\\rho_1} \\otimes \\mathbf I_{n_2}$ results in a block matrix where each block is a scalar multiple of $I_{n_2}$:\n\n$$\n\\mathbf{C_{\\rho_1}} \\otimes \\mathbf{I_{n_2}} = \\frac{1}{1-\\rho_1^2}\n\\begin{pmatrix}\n(1+\\rho_1^2)\\mathbf{I_{n_2}} & -\\rho_1\\mathbf{I_{n_2}} & \\dots & \\cdots & -\\rho_1\\mathbf{I_{n_2}} \\\\\n-\\rho_1\\mathbf{I_{n_2}} & (1+\\rho_1^2)\\mathbf{I_{n_2}} & -\\rho_1 \\mathbf{I_{n_2}} & \\cdots & \\vdots  \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n\\vdots & \\ddots & -\\rho_1\\mathbf{I_{n_2}} & (1+\\rho_1^2)\\mathbf{I_{n_2}} & -\\rho_1 \\mathbf{I_{n_2}}  \\\\\n-\\rho_1\\mathbf{I_{n_2}} & \\dots & \\cdots & -\\rho_1 \\mathbf{I_{n_2}} & (1+\\rho_1^2)\\mathbf{I_{n_2}}\n\\end{pmatrix}.\n$$\n\nSimilarly, $\\mathbf I_{n_1} \\otimes \\mathbf C_{\\rho_2}$ results in a block diagonal matrix where each block is a copy of $C_{\\rho_2}$:\n\n$$\n\\mathbf{I_{n_1}} \\otimes \\mathbf{C_{\\rho_2}} = \n\\begin{pmatrix}\n\\mathbf{C_{\\rho_2}} & \\mathbf{0} & \\cdots & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{C_{\\rho_2}} & \\cdots & \\mathbf{0} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{0} & \\mathbf{0} & \\cdots & \\mathbf{C_{\\rho_2}}\n\\end{pmatrix}.\n$$\n\nThe sum of these two matrices is a block-circulant matrix with circulant blocks:\n\n$$\n\\mathbf{Q} \\approx \\mathbf C_{\\rho_1} \\otimes \\mathbf I_{n_2} + \\mathbf I_{n_1} \\otimes \\mathbf C_{\\rho_2} = \n\\begin{pmatrix}\n\\mathbf{B}_0 & \\mathbf{B}_1 & \\cdots & \\mathbf{B}_{n_1-1} \\\\\n\\mathbf{B}_{n_1-1} & \\mathbf{B}_0 & \\cdots & \\mathbf{B}_{n_1-2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{B}_1 & \\mathbf{B}_2 & \\cdots & \\mathbf{B}_0\n\\end{pmatrix}\n$$\n\nwhere each $\\mathbf{B_i}$ is a circulant matrix. Specifically,\n\n$$\n\\begin{aligned}\n\\mathbf{B_0} &= \\frac{(1+\\rho_1^2)}{(1 - \\rho_1^2)}\\mathbf{I_{n_2}} + \\mathbf C_{\\rho_2}, \\quad \\text{and} \\\\\n\\mathbf{B_1} &= \\mathbf{B_{n_1 - 1}} = \\frac{-\\rho_1}{(1 - \\rho_1^2)}\\mathbf{I_{n_2}}.\n\\end{aligned}\n$$\n\nThis BCCB structure allows us to use 2D FFT for efficient computations. The base matrix $\\mathbf c$ for this BCCB structure is:\n\n$$\n\\mathbf{c} = \\begin{bmatrix}\n\\frac{(1+\\rho_1^2)}{(1 - \\rho_1^2)} + \\frac{(1+\\rho_2^2)}{(1 - \\rho_2^2)} & \\frac{-\\rho_1}{(1 - \\rho_1^2)} & 0 & \\cdots  & \\frac{-\\rho_1}{(1 - \\rho_1^2)} \\\\\n\\frac{-\\rho_2}{(1 - \\rho_2^2)} & 0 & 0 & \\cdots  & 0 \\\\\n0 & 0 & 0 & \\cdots  & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots &  \\vdots \\\\\n\\frac{-\\rho_2}{(1 - \\rho_2^2)} & 0 & 0 & \\cdots  & 0\n\\end{bmatrix}\n$$\n\nThis base matrix $c$ captures the structure of the precision matrix $\\mathbf Q$ and allows for efficient computation of eigenvalues using the 2D Fast Fourier Transform (FFT), enabling rapid calculation of the log-determinant and quadratic forms needed for the Gaussian copula density.\n\n## Computation with Circulant Approximation\n\nWhen using the circulant approximation, we leverage the efficient computation properties of block circulant matrices with circulant blocks (BCCB). This approach significantly reduces the computational complexity, especially for large spatial fields. Here's the step-by-step process:\n\n### 1. Construct the Base Matrix\n\nFirst, we construct the base matrix $\\mathbf c$ for our BCCB approximation of $\\mathbf Q$. For an $n_1 \\times n_2$ grid, $\\mathbf c$ is an $n_2 \\times n_1$ matrix:\n\n$$\n\\mathbf{c} = \\begin{bmatrix}\n\\frac{(1+\\rho_1^2)}{(1 - \\rho_1^2)} + \\frac{(1+\\rho_2^2)}{(1 - \\rho_2^2)} & \\frac{-\\rho_1}{(1 - \\rho_1^2)} & 0 & \\cdots  & \\frac{-\\rho_1}{(1 - \\rho_1^2)} \\\\\n\\frac{-\\rho_2}{(1 - \\rho_2^2)} & 0 & 0 & \\cdots  & 0 \\\\\n0 & 0 & 0 & \\cdots  & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots &  \\vdots \\\\\n\\frac{-\\rho_2}{(1 - \\rho_2^2)} & 0 & 0 & \\cdots  & 0\n\\end{bmatrix}\n$$\n\nThis base matrix encapsulates the structure of our Matérn-like precision matrix.\n\n### 2. Compute Initial Eigenvalues\n\nWe compute the initial eigenvalues of $\\mathbf Q$ using the 2D Fast Fourier Transform (FFT) of $\\mathbf c$:\n\n$$\n\\boldsymbol{\\Lambda} = \\text{FFT2}(\\mathbf{c})^{\\nu+1}\n$$\n\nwhere ν is the smoothness parameter.\n\n### 3. Compute Marginal Variance and Rescale Eigenvalues\n\nAn important property of Block Circulant with Circulant Blocks (BCCB) matrices is that the inverse of a BCCB matrix is also a BCCB matrix, and the marginal variance is the first element in its first circulant block. We use this to efficiently compute the marginal variance and rescale the eigenvalues:\n\na. Compute the element-wise inverse of $\\boldsymbol{\\Lambda}$: $\\mathbf{\\Lambda^{inv}} = 1 / \\boldsymbol{\\Lambda}$\nb. Compute the base of $\\mathbf Q^{-1}$ using inverse 2D FFT: $\\mathbf{c_{inv}} = \\text{IFFT2}(\\mathbf{{\\Lambda^{inv}}})$\nc. The marginal variance is given by the first element of $\\mathbf{c^{inv}}$: $\\sigma^2 = \\mathbf{c^{inv}}_{(0,0)}$\nd. Rescale the eigenvalues: $\\boldsymbol{\\widetilde \\Lambda} = \\sigma^2 \\boldsymbol{\\Lambda}$\n\nThis process ensures that the resulting precision matrix will have unit marginal variances, as required for the Gaussian copula.\n\n### 4. Compute Log-Determinant\n\nThe log-determinant of the scaled $\\mathbf{\\widetilde Q}$ can be efficiently calculated as the sum of the logarithms of the scaled eigenvalues:\n\n$$\n\\log|\\mathbf{Q}| = \\sum_{i,j} \\log(\\widetilde \\Lambda_{ij})\n$$\n\n### 5. Compute Quadratic Form\n\nTo compute the quadratic form $\\mathbf{z}^T\\mathbf{Q}\\mathbf{z}$, we use the following steps:\n\na. Compute the 2D FFT of z: $\\mathbf{\\hat{z}} = \\text{FFT2}(\\mathbf{z})$\nb. Multiply element-wise with the scaled eigenvalues: $\\mathbf{\\hat{y}} = \\boldsymbol{\\widetilde \\Lambda} \\odot \\mathbf{\\hat{z}}$\nc. Compute the inverse 2D FFT: $\\mathbf{y} = \\text{IFFT2}(\\mathbf{\\hat{y}})$\nd. Compute the dot product: $\\mathbf{z}^T\\mathbf{Q}\\mathbf{z} = \\mathbf{z}^T\\mathbf{y}$\n\n### 6. Compute the Log-Density\n\nFinally, we can compute the log-density of the Gaussian copula:\n\n$$\n\\log c(\\mathbf{u}) = \\frac{1}{2}\\log|\\mathbf{Q}| - \\frac{1}{2}\\mathbf{z}^T\\mathbf{Q}\\mathbf{z} + \\frac{1}{2}\\mathbf{z}^T\\mathbf{z}\n$$\n\nwhere $\\mathbf{z} = \\Phi^{-1}(\\mathbf{u})$.\n\n## Computation with Folded Circulant Approximation\n\nThe folded circulant approximation offers an alternative approach that can provide better accuracy near the edges of the spatial field. This method is based on the idea of reflecting the data along each coordinate axis, effectively doubling the size of the field. Other than that, the algorithmic implementation is the same except that the circulant approximation matrices to $\\mathbf Q_{\\rho}$ are now $2n \\times 2n$.\n\nFirst, we reflect the data along each coordinate axis. For a 2D spatial field represented by an $n \\times n$ matrix, the reflected data takes the form:\n\n$$\n\\begin{bmatrix}\nx_{11} & \\cdots & x_{1n_2} & x_{1n_2} & \\cdots & x_{11} \\\\\n\\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n_11} & \\cdots & x_{n_1n_2} & x_{n_1n_2} & \\cdots & x_{n_11} \\\\\nx_{n_11} & \\cdots & x_{n_1n_2} & x_{n_1n_2} & \\cdots & x_{n_11} \\\\\n\\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{11} & \\cdots & x_{1n_2} & x_{1n_2} & \\cdots & x_{11}\n\\end{bmatrix}\n$$\n\nThis reflection creates a $2n_1 \\times 2n_2$ matrix. The matrix is then stacked in lexicographic order before entering into the quadratic forms.\n\n\n\n# Results\n\n## Computational Efficiency\n\nTable 1 presents the results of a benchmark comparing the time it takes to evaluate the gaussian copula density described above. For each grid size, we report the computation time for the exact method and the two approximations, along with the speed-up factor relative to the exact method. Each calculation was performed twenty times and the median times are shown in the table. The Cholesky method is described in the appendix.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nread_csv(\"tables/benchmark_scaled.csv\") |> \ngt() |> \n  cols_label(\n    Grid = \"Grid Size\",\n    eig = \"Time\",\n    sp_1 = \"Speed-up\",\n    circ = \"Time\",\n    sp_2 = \"Speed-Up\",\n    fol = \"Time\",\n    sp_3 = \"Speed-Up\"\n  ) |> \n  tab_spanner(\n    label = \"Eigen\",\n    columns = 3:4\n  ) |> \n  tab_spanner(\n    label = \"Circulant\",\n    columns = 5:6\n  ) |> \n  tab_spanner(\n    label = \"Folded\",\n    columns = 7:8\n  ) |> \n  tab_caption(\n    md(\"Benchmarking how long it takes to evaluate the density of a Mátern($\\\\nu$)-like field with correlation parameter $\\\\rho$ scaled to have unit marginal variance\")\n  )\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 10 Columns: 8\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (8): Grid, Cholesky, eig, sp_1, circ, sp_2, fol, sp_3\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{llllllll}\n\\toprule\n &  & \\multicolumn{2}{c}{Eigen} & \\multicolumn{2}{c}{Circulant} & \\multicolumn{2}{c}{Folded} \\\\ \n\\cmidrule(lr){3-4} \\cmidrule(lr){5-6} \\cmidrule(lr){7-8}\nGrid Size & Cholesky & Time & Speed-up & Time & Speed-Up & Time & Speed-Up \\\\ \n\\midrule\\addlinespace[2.5pt]\n10x10 & 73.04µs & 173.43µs & 0.42x & 31.9µs & 2.29x & 44.16µs & 1.65x \\\\ \n20x20 & 1.43ms & 267.01µs & 5.36x & 44µs & 32.54x & 151.74µs & 9.44x \\\\ \n30x30 & 9.88ms & 808.5µs & 12.22x & 110.2µs & 89.63x & 243.07µs & 40.64x \\\\ \n40x40 & 37.88ms & 2.06ms & 18.35x & 141.5µs & 267.64x & 393.79µs & 96.18x \\\\ \n50x50 & 106.28ms & 4.56ms & 23.33x & 188.5µs & 563.82x & 604.87µs & 175.7x \\\\ \n60x60 & 253.66ms & 8.79ms & 28.86x & 242.4µs & 1046.59x & 836.07µs & 303.4x \\\\ \n70x70 & 538.42ms & 16.81ms & 32.04x & 333.1µs & 1616.37x & 1.17ms & 459.89x \\\\ \n80x80 & 1.02s & 29.59ms & 34.63x & 393.9µs & 2601.89x & 1.59ms & 643.46x \\\\ \n90x90 & 1.81s & 48.56ms & 37.27x & 598.9µs & 3022.23x & 2.04ms & 888.28x \\\\ \n100x100 & 3.09s & 76.03ms & 40.59x & 593µs & 5204.38x & 2.42ms & 1276.46x \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n\n\n\n\n# Appendix {.appendix}\n\n## Cholesky Methods\n\nStandard methods of evaluating multivariate normal densities using the Cholesky decomposition were implemented to compare with the new methods for benchmarking.\n\n### Unscaled Precision Matrix\n\n#### Precision Matrix Construction\n\nWe start by constructing the precision matrix $Q$ for a 2D Matérn field on a grid of size $d_x \\times d_y$:\n\n$$\nQ = Q_1 \\otimes I_{d_y} + I_{d_x} \\otimes Q_2 \n$$\n\nwhere $\\otimes$ denotes the Kronecker product, $Q_1$ and $Q_2$ are 1D precision matrices for the x and y dimensions respectively (typically AR(1)-like structures), and $I_{d_x}$ and $I_{d_y}$ are identity matrices of appropriate sizes.\n\n#### Density Computation\n\nFor a Matérn field with smoothness parameter $\\nu$, we need to work with $Q^{\\nu+1}$. We can efficiently compute the log-determinant, $\\log|Q^{\\nu+1}|$, and the quadratic form, $x^T Q^{\\nu+1} x$, without explicitly forming $Q^{\\nu+1}$. To do this, we compute the Cholesky decomposition, $Q = LL^T$, where L is a lower triangular matrix, and make use of the following equations:\n\n$$\n\\log|Q^{\\nu+1}| = (\\nu+1)\\log|Q| = 2(\\nu+1)\\sum_{i}\\log(L_{ii}), \n$$\n\n$$\n\\begin{aligned}\nx^T Q x &= x^T L L^T x = ||L^T x||_2^2 \\\\\nx^T Q^2 x &=  x^T L L^T L L^T x = ||LL^T x||_2^2 \\\\\nx^T Q^3 x &=  x^T L L^T L L^T L L^T x = ||L^TLL^T x||_2^2.\n\\end{aligned}\n$$\n\n\n#### Algorithm\n\n1. Construct $Q = Q_1 \\otimes I_{d_y} + I_{d_x} \\otimes Q_2$\n2. Compute Cholesky decomposition $Q = LL^T$\n3. Compute log-determinant: $\\log|Q^{\\nu+1}| = 2(\\nu+1)\\sum_{i}\\log(L_{ii})$\n4. For each observation $x$:\n    i) Initialize $y = x$\n    ii) For $j$ from 0 to $\\nu$:\n        - If $j$ is even: $y = L^T y$\n        - If $j$ is odd: $y = L y$\n    iii) Compute quadratic form $q = y^Ty$\n5. Compute log-density: $\\log p(x) = -\\frac{1}{2}(d\\log(2\\pi) + \\log|Q^{\\nu+1}| + q)$\n\n### Scaled Precision Matrix\n\n#### Precision Matrix Construction\n\nWe start by constructing the precision matrix $Q$ for a 2D Matérn field on a grid of size $d_x \\times d_y$:\n\n$$\nQ = Q_1 \\otimes I_{d_y} + I_{d_x} \\otimes Q_2 \n$$\n\nwhere $\\otimes$ denotes the Kronecker product, $Q_1$ and $Q_2$ are 1D precision matrices for the x and y dimensions respectively (typically AR(1)-like structures), and $I_{d_x}$ and $I_{d_y}$ are identity matrices of appropriate sizes. We will then have to work with the matrix $Q^{\\nu + 1}$.\n\n\nTo ensure unit marginal variances, we need to scale this precision matrix. Let $D$ be a diagonal matrix where $D_{ii} = \\sqrt{\\Sigma_{ii}}$, and $\\Sigma = (Q^{\\nu+1})^{-1}$. The scaled precision matrix is then:\n$$\n\\tilde{Q} = DQ^{\\nu+1}D\n$$\n\n#### Efficient Computation of Scaling Matrix D\n\n1. Compute the Cholesky decomposition of the original $Q = LL^T$\n2. Compute $R = L^{-1}$, so that $S = Q^{-1} = R^TR$. \n3. We then calculate the entries in $D$ using the following steps:\n    i. For $\\nu = 0$, $D_{ii} = \\sqrt{\\Sigma_{ii}} = \\sqrt{\\sum_j (R_{ji})^2}$, the column-wise norm of $R$.\n    ii. For $\\nu = 1$, we use the column-wise norm of $R^TR$\n    iii. For $\\nu = 2$, we use the column-wise norm of $RR^TR$\n\n#### Log determinant\n\n1. First, note that $\\log|\\tilde{Q}| = \\log|DQ^{\\nu+1}D| = 2\\log|D| + \\log|Q^{\\nu+1}|$\n2. We can compute $\\log|D|$ directly from the diagonal elements of D, i.e. $\\log|D| = \\sum_i \\log(D_{ii})$\n3. For $\\log|Q^{\\nu+1}|$, we can use the properties of the Cholesky decomposition: $\\log|Q^{\\nu+1}| = (\\nu+1)\\log|Q| = (\\nu+1)\\log|LL^T| = 2(\\nu+1)\\sum_i \\log(L_{ii})$\n4. Combining these, we get $\\log|\\tilde{Q}| = 2\\sum_i \\log(D_{ii}) + 2(\\nu+1)\\sum_i \\log(L_{ii})$\n\n#### Quadratic Form\n\n1. First, note that $z^T\\tilde{Q}z = z^TDQ^{\\nu+1}Dz = (Dz)^TQ^{\\nu+1}(Dz)$\n2. Let $y = Dz$. We can compute this element-wise as $y_i = D_{ii}z_i$\n3. Now we compute $y^TQ^{\\nu+1}y$ as in the unscaled case.\n\n#### Algorithm\n\nPutting it all together, here's the algorithm for computing the log-density of the Gaussian copula using the scaled precision matrix:\n\n1. Construct $Q = Q_1 \\otimes I_{d_y} + I_{d_x} \\otimes Q_2$\n2. Compute Cholesky decomposition $Q = LL^T$\n3. Compute $R = L^{-1}$ and use it to compute D as described earlier\n4. Compute log-determinant: $\\log|\\tilde{Q}| = 2\\sum_i \\log(D_{ii}) + 2(\\nu+1)\\sum_i \\log(L_{ii})$\n5. For each observation $z = \\Phi^{-1}(u)$:\n    i) Compute $y = Dz$\n    ii) Compute $y^TQ^{\\nu+1}y$ as in the unscaled case.\n6. Compute log-density: $\\log c(u) = -\\frac{1}{2}(d\\log(2\\pi) + \\log|\\tilde{Q}| + q - z^Tz)$",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{caption}\n\\usepackage{longtable}\n\\usepackage{colortbl}\n\\usepackage{array}\n\\usepackage{anyfontsize}\n\\usepackage{multirow}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}