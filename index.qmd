---
title: "Efficient Gaussian Copula Density Computation for Large-Scale Spatial Data: A Matérn-like GMRF Approach with Circulant and Folded Circulant Approximations"
author: 
  name: Brynjólfur Gauti Guðrúnar Jónsson
  affiliation: University of Iceland
  email: brynjolfur@hi.is
  url: bggj.is
date: 2024/08/15
number-sections: false
code-block-border-left: true
abstract: "This paper presents an algorithm for efficient computation of Gaussian copula densities using Gaussian Markov Random Field (GMRF) precision structures. We introduce a Matérn-like precision matrix with unit marginal variance, leveraging a Kronecker sum structure that allows for fast eigendecomposition. The method avoids explicit formation and inversion of large precision matrices, making it particularly suitable for high-dimensional spatial data. We propose two approximation methods: circulant and folded circulant, which utilize the computational efficiency of Fast Fourier Transforms (FFTs). The circulant approximation treats the spatial field as if it were on a torus, where opposite edges are connected. The folded circulant approximation, however, uses a reflection boundary condition, effectively doubling the size of the field by reflecting the data along each coordinate axis. This approach potentially offers improved accuracy near field edges, addressing a common limitation of periodic boundary conditions."
---

# Introduction

## Problem Formulation

Consider a spatial field on a regular $n \times n$ grid. Our objective is to compute the Gaussian copula density efficiently for this field. This computation involves:

1. Specifying a precision matrix $\mathbf{Q}$ that represents the spatial dependence structure.
2. Ensuring the implied covariance matrix $\mathbf{\Sigma} = \mathbf{Q}^{-1}$ has unit diagonal elements.
3. Computing the density for large spatial fields in a computationally efficient manner.

## Review

Gaussian Markov Random Fields (GMRFs) and copulas are two powerful statistical tools, each offering unique strengths in modeling complex data structures. GMRFs excel in capturing spatial and temporal dependencies, particularly in fields such as environmental science, epidemiology, and image analysis. Their ability to represent local dependencies through sparse precision matrices makes them computationally attractive for high-dimensional problems. Copulas, on the other hand, provide a flexible framework for modeling multivariate dependencies, allowing separate specification of marginal distributions and their joint behavior.

The Gaussian copula, in particular, has gained popularity due to its interpretability and connection to the multivariate normal distribution. However, combining GMRFs with copulas has historically been computationally challenging, limiting their joint application to smaller datasets or simpler models.

Let $\mathbf{X} = (X_1, X_2, \ldots, X_n)$ be a multivariate random vector with marginal distribution functions $F_i$ for $i = 1, 2, \ldots, n$. The joint distribution function of $\mathbf{X}$ can be written as:

$$
F_{\mathbf{X}}(\mathbf{x}) = C(F_1(x_1), F_2(x_2), \ldots, F_n(x_n)),
$$

where $C$ is the Gaussian copula defined by the GMRF precision matrix $\mathbf{Q}$. The Gaussian copula $C$ is given by:

$$
C(u_1, u_2, \ldots, u_n) = \Phi_\mathbf{Q}(\Phi^{-1}(u_1), \Phi^{-1}(u_2), \ldots, \Phi^{-1}(u_n)),
$$

where $\Phi_\mathbf{Q}$ is the joint cumulative distribution function of a multivariate normal distribution with mean vector $\mathbf{0}$ and precision matrix $\mathbf{Q}$, and $\Phi^{-1}$ is the inverse of the standard normal cumulative distribution function.

A critical requirement for the precision matrix $\mathbf{Q}$ governing the GMRF copula $C$ is that $\mathbf{\Sigma} = \mathbf{Q}^{-1}$ should have a unit diagonal, i.e. the marginal variance is equal to one everywhere.. This ensures it operates on the same scale as the transformed data, $\Phi^{-1}(u_i)$. However, this can be challenging as GMRFs are typically defined in terms of precision matrices that often imply non-unit marginal variances.

This paper presents a novel algorithm that bridges the gap between GMRFs and copulas, allowing for fast and efficient computation of Gaussian copula densities using GMRF precision structures. Our method focuses on creating a Matérn-like precision matrix $\mathbf{Q}$ with unit marginal variance and efficiently computing the multivariate Gaussian copula density of $\mathbf{Z} = \Phi^{-1}(\mathbf{u})$, where $u_i \sim \text{Uniform}(0, 1)$, $i = 1, \dots, n$.

The key innovation lies in leveraging the special structure of the precision matrix:

$$
\mathbf{Q} = \mathbf{Q}_1 \otimes \mathbf{I} + \mathbf{I} \otimes \mathbf{Q}_1,
$$

where $\mathbf{Q}_1$ is the precision matrix of a standardized one-dimensional AR(1) process and $\otimes$ denotes the Kronecker product. By employing efficient eigendecomposition techniques, our method avoids explicit formation and inversion of the large precision matrix $\mathbf{Q}$, making it particularly suitable for high-dimensional spatial data. In additions to the exact method, we mention approximations to $\mathbf{Q}$ using circuland and folded circulant matrices.

# Methods

## Gaussian Copula Density Computation

The Gaussian copula density for a random vector $\mathbf{U} = (U_1, ..., U_n)$ with $U_i \sim \text{Uniform}(0,1)$ is given by:

$$
c(\mathbf{u}) = |\mathbf{Q}|^{1/2} \exp\left(-\frac{1}{2}\mathbf{z}^T(\mathbf{Q} - \mathbf{I})\mathbf{z}\right)
$$

where $\mathbf{z} = (z_1, ..., z_n)$ with $z_i = \Phi^{-1}(u_i)$, $\mathbf{Q}$ is the precision matrix, and $\mathbf{I}$ is the identity matrix.

The log-density can be expressed as:

$$
\log c(\mathbf{u}) = \frac{1}{2}\log|\mathbf{Q}| - \frac{1}{2}\mathbf{z}^T\mathbf{Q}\mathbf{z} + \frac{1}{2}\mathbf{z}^T\mathbf{z}
$$

Our goal is to efficiently compute this log-density for large spatial fields.

## Precision Matrix Structure

We define the precision matrix $\mathbf{Q}$ as:

$$
\mathbf{Q} = (\mathbf{Q}_1 \otimes \mathbf{I} + \mathbf{I} \otimes \mathbf{Q}_1)^{(\nu + 1)}, \quad \nu \in \{0, 1, 2\}
$$

where $\mathbf{Q}_1$ is the precision matrix of a one-dimensional AR(1) process:

$$
\mathbf{Q}_1 = \frac{1}{1-\rho^2}
\begin{bmatrix}
1 & -\rho & 0 & \cdots & 0 \\
-\rho & 1+\rho^2 & -\rho & \cdots & 0 \\
0 & -\rho & 1+\rho^2 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{bmatrix}
$$

The matrix, $\mathbf Q$, is then scaled so that its inverse, $\mathbf \Sigma = \mathbf Q^{-1}$ has unit diagonals, i.e. $\mathbf \Sigma_{ii} = 1$.

## Computation Process

### Step 1: Eigendecomposition of $\mathbf{Q}_1$

We first compute the eigendecomposition of $\mathbf{Q}_1$:

$$
\mathbf{Q}_1 = \mathbf{V}\mathbf{\Lambda}\mathbf{V}^T
$$

where $\mathbf{V}$ is the matrix of eigenvectors and $\mathbf{\Lambda}$ is the diagonal matrix of eigenvalues. Then, the eigendecomposition of $\mathbf{Q}$ is:

$$
\mathbf{Q} = (\mathbf{V} \otimes \mathbf{V})(\mathbf{\Lambda} \otimes \mathbf{I} + \mathbf{I} \otimes \mathbf{\Lambda})^{(\nu + 1)}(\mathbf{V} \otimes \mathbf{V})^T.
$$

This means that the eigenvectors of $\mathbf{Q}$ are $\mathbf{v}_j \otimes \mathbf{v}_i$ and the corresponding eigenvalues are $\lambda_i + \lambda_j$.

### Step 2: Computation of Marginal Standard Deviations

Using the eigendecomposition of $\mathbf{Q}_1$, we compute the marginal standard deviations and store them in a vector $\mathbf{\sigma}$, i.e. $\sigma_i = \sqrt{\Sigma_{ii}}$:

$$
\mathbf{\sigma} = \sqrt{\sum_{i,j} \frac{(\mathbf{v}_j \otimes \mathbf{v}_i)^2}{(\lambda_i + \lambda_j)^\nu}}
$$

where $\mathbf{v}_i$ are the eigenvectors and $\lambda_i$ are the eigenvalues of $\mathbf{Q}_1$. Thus, $\sigma$ will be a vector of length $n^2$.

### Step 3: Scaling the Eigendecomposition

We scale the eigendecomposition of $\mathbf{Q}$ using the marginal standard deviations:

$$
\begin{aligned}
\mathbf{\widetilde  Q} &= \mathbf{D}\mathbf{Q}\mathbf{D} \\
&= \mathbf{D}(\mathbf{V} \otimes \mathbf{V})(\mathbf{\Lambda} \otimes \mathbf{I} + \mathbf{I} \otimes \mathbf{\Lambda})^\nu(\mathbf{V} \otimes \mathbf{V})^T\mathbf{D} \\
&= (\mathbf{\widetilde V} \otimes \mathbf{\widetilde V})(\mathbf{\widetilde\Lambda} \otimes \mathbf{I} + \mathbf{I} \otimes \mathbf{\widetilde \Lambda})(\mathbf{\widetilde V} \otimes \mathbf{\widetilde V})^T
\end{aligned}
$$

where $\mathbf{D}$ is a diagonal matrix with $D_{ii} = \sigma_i$.

### Step 4: Efficient Computation of Log-Density

Using this scaled eigendecomposition, we efficiently compute:

1. Log-determinant: $\log|\mathbf{\widetilde Q}| = \sum_{i,j} \log(\widetilde\lambda_i + \widetilde\lambda_j)$

2. Quadratic form: $\mathbf{z}^T\mathbf{\widetilde Q}\mathbf{z} = \sum_{i,j} (\widetilde\lambda_i + \widetilde\lambda_j) y_{ij}^2$, where $y_{ij} = (\mathbf{\widetilde v}_j \otimes \mathbf{\widetilde v}_i)^T\mathbf{z}$

This approach allows for efficient computation of the Gaussian copula density without explicitly forming the full $n^2 \times n^2$ precision matrix $\mathbf{Q}$ or its inverse $\mathbf{\Sigma}$.

## Circulant and Folded Circulant Approximations

While the eigendecomposition method provides an exact solution, it can be computationally expensive for very large spatial fields. To address this, we introduce circulant and folded circulant approximations that offer potential computational advantages.

### Circulant Matrices

A circulant matrix $C$ is a special kind of matrix where each row is a cyclic shift of the row above it. It can be fully specified by its first row or column, called the base $c$:

$$
C = \begin{pmatrix}
c_0 & c_1 & c_2 & \cdots & c_{n-1} \\
c_{n-1} & c_0 & c_1 & \cdots & c_{n-2} \\
c_{n-2} & c_{n-1} & c_0 & \cdots & c_{n-3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
c_1 & c_2 & c_3 & \cdots & c_0
\end{pmatrix} = (c_{j-i \mod n})
$$

The base vector $c$ completely determines the circulant matrix and plays a crucial role in efficient computations. In particular:

1. The eigenvalues of $C$ are given by the Discrete Fourier Transform (DFT) of $c$:
$$
\lambda_k = \sum_{j=0}^{n-1} c_j e^{-2\pi i jk/n}, \quad k = 0, 1, ..., n-1
$$

2. Matrix-vector multiplication can be performed using the FFT:
$$
Cv = \frac1n\text{DFT}(\text{DFT}(c) \odot \text{IDFT}(v))
$$

3. The determinant of $C$ is the product of its eigenvalues:
$$
\det(C) = \prod_{k=0}^{n-1} \lambda_k
$$

4. When $C$ is non singular, then the inverse is circulant and thus determined by its base:

$$
\frac1n \text{IDFT}(\text{DFT}(c)^{-1}).
$$

These properties allow for much faster computations than for general matrices.


### Block Circulant Matrices

For two-dimensional spatial fields, we use block circulant matrices with circulant blocks (BCCB). An $Nn \times Nn$ matrix C is block circulant if it has the form:

$$
C = \begin{pmatrix}
C_0 & C_1 & C_2 & \cdots & C_{N-1} \\
C_{N-1} & C_0 & C_1 & \cdots & C_{N-2} \\
C_{N-2} & C_{N-1} & C_0 & \cdots & C_{N-3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
C_1 & C_2 & C_3 & \cdots & C_0
\end{pmatrix} = (C_{j-i \mod N})
$$

where each $C_i$ is itself a circulant $n \times n$ matrix.

For a BCCB matrix, we define a base matrix $\mathbf c$, which is an $n \times N$ matrix where each column is the base vector of the corresponding circulant block. This base matrix $\mathbf c$ completely determines the BCCB matrix and is central to efficient computations:

1. The eigenvalues of $C$ are given by the 2D DFT of $\mathbf c$:
$$
\Lambda_{k,l} = \sum_{i=0}^{n-1} \sum_{j=0}^{N-1} c_{ij} e^{-2\pi i (ki/n + lj/N)}, \quad k = 0, \dots, n-1, l = 0, \dots, N-1
$$

2. Matrix-vector multiplication can be performed using the 2D FFT:
$$
Cv = \text{IDFT2}(\text{DFT2}(c) \odot \text{DFT2}(v))
$$

3. The determinant of $C$ is the product of its eigenvalues:
$$
\det(C) = \prod_{k=0}^{n-1} \prod_{l=0}^{N-1} \Lambda_{k,l}
$$


### Computational Advantages

The circulant structure allows for efficient computation using the Fast Fourier Transform (FFT):

1. Matrix-vector multiplication: For a circulant matrix C with base c and a vector v
$$
Cv = \sqrt{n} \text{DFT}(\text{DFT}(c) \odot \text{IDFT}(v)),
$$

2. Matrix inverse: The base of $C^{-1}$ is given by
$$
\frac{1}{n} \text{IDFT}(\text{DFT}(c)^{-1}).
$$

### Approximations for Q1

Let $Q_1$ be the precision matrix of a one-dimensional AR(1) process with n observations. The exact form of $Q_1$ is:

$$
\mathbf{Q}_1 = \frac{1}{1-\rho^2}
\begin{bmatrix}
1 & -\rho & 0 & \cdots & 0 \\
-\rho & 1+\rho^2 & -\rho & \cdots & 0 \\
0 & -\rho & 1+\rho^2 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{bmatrix}
$$

#### Circulant Approximation

The circulant approximation to $Q_1$, denoted as $\mathbf{Q}_1^{(circ)}$, is:

$$
\mathbf{Q}_1^{(circ)} = \frac{1}{1-\rho^2}
\begin{bmatrix}
1+\rho^2 & -\rho & 0 & \cdots & 0 & -\rho \\
-\rho & 1+\rho^2 & -\rho & \cdots & 0 & 0 \\
0 & -\rho & 1+\rho^2 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
-\rho & 0 & 0 & \cdots & -\rho & 1+\rho^2
\end{bmatrix}
$$

This approximation treats the first and last observations as neighbors, effectively wrapping the data around a circle.

#### Folded Circulant Approximation

The folded circulant approximation, $\mathbf{Q}_1^{(fold)}$, is based on a reflected version of the data. We double the data by reflecting it, giving us the data $x_1,  \dots, x_n, x_n, \dots, x_1$. We then model this doubled data with a $2n \times 2n$ circulant matrix. If written out as an $n \times n$ matrix, it takes the form:

$$
\mathbf{Q}_1^{(fold)} = \frac{1}{1-\rho^2}
\begin{bmatrix}
1-\rho+\rho^2 & -\rho & 0 & \cdots & 0 & 0 \\
-\rho & 1+\rho^2 & -\rho & \cdots & 0 & 0 \\
0 & -\rho & 1+\rho^2 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & -\rho & 1-\rho+\rho^2
\end{bmatrix}
$$

This approximation modifies the first and last diagonal elements to account for the reflection of the data. As $x_1$ now is the first and last data point, then we avoid the circular dependence from the regular circulant approximation.

### Extension to the Full Q Matrix

For a two-dimensional spatial field on an $n \times n$ grid, we construct the full precision matrix Q using a Kronecker sum:

$$
\mathbf{Q} = (\mathbf{Q}_1 \otimes \mathbf{I} + \mathbf{I} \otimes \mathbf{Q}_1)^{(\nu + 1)}
$$

where $\otimes$ denotes the Kronecker product, I is the $n \times n$ identity matrix, and $\nu$ is a smoothness parameter.

When we approximate $Q_1$ with a circulant matrix, this Kronecker sum results in a block-circulant matrix with circulant blocks (BCCB). To see this, let's consider the case where $\nu = 0$ for simplicity:

$$
\mathbf{Q} = \mathbf{Q}_1 \otimes \mathbf{I} + \mathbf{I} \otimes \mathbf{Q}_1
$$

Now, let $Q_1$ be approximated by a circulant matrix C with base vector $c = [c_, c_1, ..., c_{n-1}]$. Then:

$$
\mathbf{Q}_1 \approx \mathbf{C} = 
\begin{pmatrix}
c_0 & c_1 & \cdots & c_{n-1} \\
c_{n-1} & c_0 & \cdots & c_{n-2} \\
\vdots & \vdots & \ddots & \vdots \\
c_1 & c_2 & \cdots & c_0
\end{pmatrix}
$$

The Kronecker product $C \otimes I$ results in a block matrix where each block is a scalar multiple of I:

$$
\mathbf{C} \otimes \mathbf{I} = 
\begin{pmatrix}
c_0\mathbf{I} & c_1\mathbf{I} & \cdots & c_{n-1}\mathbf{I} \\
c_{n-1}\mathbf{I} & c_0\mathbf{I} & \cdots & c_{n-2}\mathbf{I} \\
\vdots & \vdots & \ddots & \vdots \\
c_1\mathbf{I} & c_2\mathbf{I} & \cdots & c_0\mathbf{I}
\end{pmatrix}
$$

Similarly, I ⊗ C results in a block matrix where each block is a copy of C:

$$
\mathbf{I} \otimes \mathbf{C} = 
\begin{pmatrix}
\mathbf{C} & \mathbf{0} & \cdots & \mathbf{0} \\
\mathbf{0} & \mathbf{C} & \cdots & \mathbf{0} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{0} & \mathbf{0} & \cdots & \mathbf{C}
\end{pmatrix}
$$

The sum of these two matrices is a block-circulant matrix with circulant blocks:

$$
\mathbf{Q} \approx \mathbf{C} \otimes \mathbf{I} + \mathbf{I} \otimes \mathbf{C} = 
\begin{pmatrix}
\mathbf{B}_0 & \mathbf{B}_1 & \cdots & \mathbf{B}_{n-1} \\
\mathbf{B}_{n-1} & \mathbf{B}_0 & \cdots & \mathbf{B}_{n-2} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{B}_1 & \mathbf{B}_2 & \cdots & \mathbf{B}_0
\end{pmatrix}
$$

where each $\mathbf{B_i}$ is a circulant matrix. Specifically, $\mathbf{B_0} = c_0I + C$, and for $i \neq 0$, $\mathbf{B_i} = c_iI$.

This BCCB structure allows us to use 2D FFT for efficient computations. The base matrix $\mathbf c$ for this BCCB structure is:

$$
\mathbf{c} = \begin{bmatrix}
2+2\rho^2 & -\rho & 0 & \cdots  & -\rho \\
-\rho & 0 & 0 & \cdots  & 0 \\
0 & 0 & 0 & \cdots  & 0 \\
\vdots & \vdots & \vdots & \ddots &  \vdots \\
-\rho & 0 & 0 & \cdots  & 0
\end{bmatrix}
$$

This base matrix $c$ captures the structure of the precision matrix $Q$ and allows for efficient computation of eigenvalues using the 2D Fast Fourier Transform (FFT), enabling rapid calculation of the log-determinant and quadratic forms needed for the Gaussian copula density.

## Computation with Circulant Approximation

When using the circulant approximation, we leverage the efficient computation properties of block circulant matrices with circulant blocks (BCCB). This approach significantly reduces the computational complexity, especially for large spatial fields. Here's the step-by-step process:

### 1. Construct the Base Matrix

First, we construct the base matrix c for our BCCB approximation of Q. For an n × n grid, c is an n × n matrix:

$$
\mathbf{c} = \begin{bmatrix}
2+2\rho^2 & -\rho & 0 & \cdots & 0 & -\rho \\
-\rho & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & 0 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
-\rho & 0 & 0 & \cdots & 0 & 0
\end{bmatrix}
$$

This base matrix encapsulates the structure of our Matérn-like precision matrix.

### 2. Compute Initial Eigenvalues

We compute the initial eigenvalues of Q using the 2D Fast Fourier Transform (FFT) of c:

$$
\boldsymbol{\Lambda} = \text{FFT2}(\mathbf{c})^{\nu+1}
$$

where ν is the smoothness parameter.

### 3. Compute Marginal Variance and Rescale Eigenvalues

An important property of Block Circulant with Circulant Blocks (BCCB) matrices is that the inverse of a BCCB matrix is also a BCCB matrix with a constant diagonal. We use this to efficiently compute the marginal variance and rescale the eigenvalues:

a. Compute the element-wise inverse of $\boldsymbol{\Lambda}$: $\mathbf{\Lambda^{inv}} = 1 / \boldsymbol{\Lambda}$
b. Compute the base of $Q^{-1}$ using inverse 2D FFT: $\mathbf{c_{inv}} = \text{IFFT2}(\mathbf{{\Lambda^{inv}}})$
c. The marginal variance is given by the first element of $\mathbf{c^{inv}}$: $\sigma^2 = \mathbf{c^{inv}}_{(0,0)}$
d. Rescale the eigenvalues: $\boldsymbol{\widetilde \Lambda} = \sigma^2 \boldsymbol{\Lambda}$

This process ensures that the resulting precision matrix will have unit marginal variances, as required for the Gaussian copula.

### 4. Compute Log-Determinant

The log-determinant of the scaled $\mathbf{\widetilde Q}$ can be efficiently calculated as the sum of the logarithms of the scaled eigenvalues:

$$
\log|\mathbf{Q}| = \sum_{i,j} \log(\widetilde \Lambda_{ij})
$$

### 5. Compute Quadratic Form

To compute the quadratic form $\mathbf{z}^T\mathbf{Q}\mathbf{z}$, we use the following steps:

a. Compute the 2D FFT of z: $\mathbf{\hat{z}} = \text{FFT2}(\mathbf{z})$
b. Multiply element-wise with the scaled eigenvalues: $\mathbf{\hat{y}} = \boldsymbol{\widetilde \Lambda} \odot \mathbf{\hat{z}}$
c. Compute the inverse 2D FFT: $\mathbf{y} = \text{IFFT2}(\mathbf{\hat{y}})$
d. Compute the dot product: $\mathbf{z}^T\mathbf{Q}\mathbf{z} = \mathbf{z}^T\mathbf{y}$

### 6. Compute the Log-Density

Finally, we can compute the log-density of the Gaussian copula:

$$
\log c(\mathbf{u}) = \frac{1}{2}\log|\mathbf{Q}| - \frac{1}{2}\mathbf{z}^T\mathbf{Q}\mathbf{z} + \frac{1}{2}\mathbf{z}^T\mathbf{z}
$$

where $\mathbf{z} = \Phi^{-1}(\mathbf{u})$.

## Computation with Folded Circulant Approximation

The folded circulant approximation offers an alternative approach that can provide better accuracy near the edges of the spatial field. This method is based on the idea of reflecting the data along each coordinate axis, effectively doubling the size of the field. Other than that, the algorithmic implementation is the same except that the circulant approximation matrices to $Q_1$ are now $2n \times 2n$.

First, we reflect the data along each coordinate axis. For a 2D spatial field represented by an $n \times n$ matrix, the reflected data takes the form:

$$
\begin{bmatrix}
x_{11} & \cdots & x_{1n} & x_{1n} & \cdots & x_{11} \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
x_{n1} & \cdots & x_{nn} & x_{nn} & \cdots & x_{n1} \\
x_{n1} & \cdots & x_{nn} & x_{nn} & \cdots & x_{n1} \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
x_{11} & \cdots & x_{1n} & x_{1n} & \cdots & x_{11}
\end{bmatrix}
$$

This reflection creates a $2n \times 2n$ matrix. The matrix is then stacked in lexicographic order before entering into the quadratic forms.



# Results

## Computational Efficiency

Table 1 presents the results of a benchmark comparing the time it takes to evaluate the gaussian copula density described above. For each grid size, we report the computation time for the exact method and the two approximations, along with the speed-up factor relative to the exact method. Each calculation was performed twenty times and the median times are shown in the table.



| Q_size       | Exact    | Circulant |         | Folded    |         |
|--------------|----------|-----------|---------|-----------|---------|
|              | Time     | Time      | Speed-Up| Time      | Speed-Up|
| 100x100      | 194.69μs | 33.89μs   | 5.75x   | 44.77μs   | 4.35x   |
| 400x400      | 309.61μs | 37.45μs   | 8.27x   | 122.78μs  | 2.52x   |
| 900x900      | 790.30μs | 85.65μs   | 9.23x   | 155.47μs  | 5.08x   |
| 1600x1600    | 1.96ms   | 98.71μs   | 19.86x  | 243.64μs  | 8.05x   |
| 3600x3600    | 8.66ms   | 151.70μs  | 57.1x   | 484.07μs  | 17.89x  |
| 10000x10000  | 71.81ms  | 343.07μs  | 209.31x | 1.36ms    | 52.76x  |
| 19600x19600  | 246.40ms | 667.58μs  | 369.09x | 2.89ms    | 85.32x  |
| 40000x40000  | 997.32ms | 1.44ms    | 694.46x | 7.69ms    | 129.69x |

: Table 1. Benchmarking how long it takes to evaluate the density of a Mátern($\nu$)-like field with correlation parameter $\rho$, scaled to have unit marginal variance.

The results demonstrate significant computational gains from both approximation methods, with the efficiency advantage increasing for larger grid sizes. Key observations include:

1. **Scalability**: Both approximation methods show superior scalability compared to the exact method. As the grid size increases, the speed-up factor generally increases, indicating that the approximations become increasingly advantageous for larger spatial fields.

2. **Circulant Approximation Performance**: The circulant approximation consistently outperforms the folded circulant approximation in terms of speed. For the largest grid size (40000x40000), it achieves a remarkable 694.46x speed-up over the exact method.

3. **Folded Circulant Approximation**: While not as fast as the circulant approximation, the folded circulant method still offers substantial speed improvements, reaching a 129.69x speed-up for the 40000x40000 grid.

4. **Trade-off Considerations**: The choice between the circulant and folded circulant approximations may depend on the specific requirements of the application. While the circulant approximation is faster, the folded circulant method may offer better accuracy, particularly near the edges of the spatial field.

These results underscore the practical value of our approximation methods, especially for large-scale spatial analyses where computational efficiency is crucial. The substantial speed-ups achieved, particularly for larger grids, demonstrate the potential of these methods to enable the analysis of much larger spatial datasets than previously feasible with exact methods.


# Appendix {.appendix}

## Cholesky Methods

Standard methods of evaluating multivariate normal densities using the Cholesky decomposition were implemented to compare with the new methods for benchmarking.

### Unscaled Precision Matrix

#### Precision Matrix Construction

We start by constructing the precision matrix $Q$ for a 2D Matérn field on a grid of size $d_x \times d_y$:

$$
Q = Q_1 \otimes I_{d_y} + I_{d_x} \otimes Q_2 
$$

where $\otimes$ denotes the Kronecker product, $Q_1$ and $Q_2$ are 1D precision matrices for the x and y dimensions respectively (typically AR(1)-like structures), and $I_{d_x}$ and $I_{d_y}$ are identity matrices of appropriate sizes.

#### Density Computation

For a Matérn field with smoothness parameter $\nu$, we need to work with $Q^{\nu+1}$. We can efficiently compute the log-determinant, $\log|Q^{\nu+1}|$, and the quadratic form, $x^T Q^{\nu+1} x$, without explicitly forming $Q^{\nu+1}$. To do this, we compute the Cholesky decomposition, $Q = LL^T$, where L is a lower triangular matrix, and make use of the following equations:

$$
\log|Q^{\nu+1}| = (\nu+1)\log|Q| = 2(\nu+1)\sum_{i}\log(L_{ii}), 
$$

$$
\begin{aligned}
x^T Q x &= x^T L L^T x = ||L^T x||_2^2 \\
x^T Q^2 x &=  x^T L L^T L L^T x = ||LL^T x||_2^2 \\
x^T Q^3 x &=  x^T L L^T L L^T L L^T x = ||L^TLL^T x||_2^2.
\end{aligned}
$$


#### Algorithm

1. Construct $Q = Q_1 \otimes I_{d_y} + I_{d_x} \otimes Q_2$
2. Compute Cholesky decomposition $Q = LL^T$
3. Compute log-determinant: $\log|Q^{\nu+1}| = 2(\nu+1)\sum_{i}\log(L_{ii})$
4. For each observation $x$:
    i) Initialize $y = x$
    ii) For $j$ from 0 to $\nu$:
        - If $j$ is even: $y = L^T y$
        - If $j$ is odd: $y = L y$
    iii) Compute quadratic form $q = y^Ty$
5. Compute log-density: $\log p(x) = -\frac{1}{2}(d\log(2\pi) + \log|Q^{\nu+1}| + q)$

### Scaled Precision Matrix

#### Precision Matrix Construction

We start by constructing the precision matrix $Q$ for a 2D Matérn field on a grid of size $d_x \times d_y$:

$$
Q = Q_1 \otimes I_{d_y} + I_{d_x} \otimes Q_2 
$$

where $\otimes$ denotes the Kronecker product, $Q_1$ and $Q_2$ are 1D precision matrices for the x and y dimensions respectively (typically AR(1)-like structures), and $I_{d_x}$ and $I_{d_y}$ are identity matrices of appropriate sizes. We will then have to work with the matrix $Q^{\nu + 1}$.


To ensure unit marginal variances, we need to scale this precision matrix. Let $D$ be a diagonal matrix where $D_{ii} = \sqrt{\Sigma_{ii}}$, and $\Sigma = (Q^{\nu+1})^{-1}$. The scaled precision matrix is then:
$$
\tilde{Q} = DQ^{\nu+1}D
$$

#### Efficient Computation of Scaling Matrix D

1. Compute the Cholesky decomposition of the original $Q = LL^T$
2. Compute $R = L^{-1}$, so that $S = Q^{-1} = R^TR$. 
3. We then calculate the entries in $D$ using the following steps:
    i. For $\nu = 0$, $D_{ii} = \sqrt{\Sigma_{ii}} = \sqrt{\sum_j (R_{ji})^2}$, the column-wise norm of $R$.
    ii. For $\nu = 1$, we use the column-wise norm of $R^TR$
    iii. For $\nu = 2$, we use the column-wise norm of $RR^TR$

#### Log determinant

1. First, note that $\log|\tilde{Q}| = \log|DQ^{\nu+1}D| = 2\log|D| + \log|Q^{\nu+1}|$
2. We can compute $\log|D|$ directly from the diagonal elements of D, i.e. $\log|D| = \sum_i \log(D_{ii})$
3. For $\log|Q^{\nu+1}|$, we can use the properties of the Cholesky decomposition: $\log|Q^{\nu+1}| = (\nu+1)\log|Q| = (\nu+1)\log|LL^T| = 2(\nu+1)\sum_i \log(L_{ii})$
4. Combining these, we get $\log|\tilde{Q}| = 2\sum_i \log(D_{ii}) + 2(\nu+1)\sum_i \log(L_{ii})$

#### Quadratic Form

1. First, note that $z^T\tilde{Q}z = z^TDQ^{\nu+1}Dz = (Dz)^TQ^{\nu+1}(Dz)$
2. Let $y = Dz$. We can compute this element-wise as $y_i = D_{ii}z_i$
3. Now we compute $y^TQ^{\nu+1}y$ as in the unscaled case.

#### Algorithm

Putting it all together, here's the algorithm for computing the log-density of the Gaussian copula using the scaled precision matrix:

1. Construct $Q = Q_1 \otimes I_{d_y} + I_{d_x} \otimes Q_2$
2. Compute Cholesky decomposition $Q = LL^T$
3. Compute $R = L^{-1}$ and use it to compute D as described earlier
4. Compute log-determinant: $\log|\tilde{Q}| = 2\sum_i \log(D_{ii}) + 2(\nu+1)\sum_i \log(L_{ii})$
5. For each observation $z = \Phi^{-1}(u)$:
    i) Compute $y = Dz$
    ii) Compute $y^TQ^{\nu+1}y$ as in the unscaled case.
6. Compute log-density: $\log c(u) = -\frac{1}{2}(d\log(2\pi) + \log|\tilde{Q}| + q - z^Tz)$